#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jun  7 15:40:07 2021

@author: ken
"""

from stanfordcorenlp import StanfordCoreNLP
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
import warnings
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import pycrfsuite
import sklearn_crfsuite
from nltk.stem import WordNetLemmatizer
import sys


TrainDataPath = "/home/ken/Service-Requirement-Extraction-via-NLP/src_NN/Data_augmentation/"
TestDataPath = "/home/ken/Service-Requirement-Extraction-via-NLP/src/Leetcode_data/ExtraData/"
PreTrain_WV_Path = "/home/ken/Service-Requirement-Extraction-via-NLP/src_NN/GloVe/"

nlp = StanfordCoreNLP(r'/home/ken/stanford-corenlp-4.2.0')
lemmatizer = WordNetLemmatizer()


input_relateword_sets = [
    "input",
    "given",
    "give",
    "provide",
    "receive",
    "take"
    ]



def SentencesProcessing(raw_sentences):
    #new_sentences = raw_sentences.lower()
    new_sentences = re.sub(r'\([^)]*\)', '', raw_sentences)
    new_sentences = re.sub(r"\n","", new_sentences)
    #new_sentences = re.sub('"','', new_sentences)
    #tokens = [w for w in new_sentences.split() if not w in stop_words]
    #new_sentences = re.sub("[^a-zA-Z]", " ", new_sentences) 
    long_words=[]
    for i in new_sentences.split():
        long_words.append(i)   
    return (" ".join(long_words)).strip()

def Read_Sentences(dir_path, filename = 'sentences'):
    file = open(dir_path + filename, "r+")
    sentence_group = []
    x = file.readline()
    while(x != ""):
        sentence_group.append(SentencesProcessing(x))
        x = file.readline()
    file.close()
    return sentence_group

def POS(sentence):
    return nlp.pos_tag(sentence)

def DEP(sentence):
    return nlp.dependency_parse(sentence)

def tokenize(sentence):
    return nlp.word_tokenize(sentence)

#def is_input_keyword(token):
#    return (token in input_relateword_sets)

def is_input_keyword(token):
    return (lemmatizer.lemmatize(token, pos="v").lower() in input_relateword_sets)

def is_given(token):
    if token == 'given':
        return True
    return (lemmatizer.lemmatize(token, pos="v").lower() == 'given')

def is_input(token):
    if token == 'input':
        return True
    return (lemmatizer.lemmatize(token, pos="v").lower() == 'input')

def is_give(token):
    if token == 'give':
        return True
    return (lemmatizer.lemmatize(token, pos="v").lower() == 'give')

def is_provide(token):
    if token == 'provide':
        return True
    return (lemmatizer.lemmatize(token, pos="v").lower() == 'provide')

def is_receive(token):
    if token == 'receive':
        return True
    return (lemmatizer.lemmatize(token, pos="v").lower() == 'receive')

def is_take(token):
    if token == 'take':
        return True
    return (lemmatizer.lemmatize(token, pos="v").lower() == 'take')


def is_NN(pos):
    if pos == 'NN' or pos == 'NNS' or pos =='NNP' or pos == 'NNPS':
        return True
    else:
        return False
    
def is_DT(pos):
    if pos == 'DT':
        return True
    else:
        return False

def is_prep(token):
    if token == 'of' or token == 'for' or token =='from' or token == 'to':
        return True
    else:
        return False
    
def is_of(token):
    return token == 'of'

def is_for(token):
    return token == 'for'

def is_from(token):
    return token == 'from'

def is_to(token):
    return token == 'to'

N_gram = 3
def word2features(sent, i):
    current_word = sent[i][0]
    pos = sent[i][1]
    features = [
        'bias',
        #'word=' + current_word,
        #'word.islower={}'.format(current_word.islower()),
        #'word.isupper={}'.format(current_word.isupper()),
        #'word.is_input_keyword={}'.format(is_input_keyword(current_word)),
        #'word.is_prep={}'.format(is_prep(current_word)),
        'pos=' + pos,
        
        
        
        #'word.is_given={}'.format(is_given(current_word)),
        #'word.is_input={}'.format(is_input(current_word)),
        #'word.is_give={}'.format(is_give(current_word)),
        #'word.is_provide={}'.format(is_provide(current_word)),
        #'word.is_receive={}'.format(is_receive(current_word)),
        #'word.is_take={}'.format(is_take(current_word)),
        #'word.is_of={}'.format(is_of(current_word)),
        #'word.is_for={}'.format(is_for(current_word)),
        #'word.is_from={}'.format(is_from(current_word)),
        #'word.is_to={}'.format(is_to(current_word)),
        
        #'word.is_NN={}'.format(is_NN(pos)),
        #'word.is_DT={}'.format(is_DT(pos)),
        
    ]
    #if is_input_keyword(current_word):
    #        features.append('word.input_keyword_is=' + current_word)
    if i == 0:
        features.append('BOS')
    if i == len(sent) - 1:
        features.append('EOS')
        
    #Forward gram
    for k in range(N_gram):
        next_index = k + 1
        if i + next_index > len(sent) - 1:
            break
        target_word = sent[i+next_index][0]
        target_pos = sent[i+next_index][1]
        features.extend([
            #'+' + str(next_index) + ':word=' + target_word,
            #'+' + str(next_index) + (':word.islower={}'.format(target_word.islower())),
            #'+' + str(next_index) + (':word.isupper={}'.format(target_word.isupper())),
            '+' + str(next_index) + ':pos=' + target_pos,
            #'+' + str(next_index) + (':word.is_input_keyword={}'.format(is_input_keyword(target_word))),
            #'+' + str(next_index) + (':word.is_prep={}'.format(is_prep(target_word))),
            
            #'+' + str(next_index) + (':word.is_given={}'.format(is_given(target_word))),
            #'+' + str(next_index) + (':word.is_input={}'.format(is_input(target_word))),
            #'+' + str(next_index) + (':word.is_give={}'.format(is_give(target_word))),
            #'+' + str(next_index) + (':word.is_provide={}'.format(is_provide(target_word))),
            #'+' + str(next_index) + (':word.is_receive={}'.format(is_receive(target_word))),
            #'+' + str(next_index) + (':word.is_take={}'.format(is_take(target_word))),
            #'+' + str(next_index) + (':word.is_of={}'.format(is_of(target_word))),
            #'+' + str(next_index) + (':word.is_for={}'.format(is_for(target_word))),
            #'+' + str(next_index) + (':word.is_from={}'.format(is_from(target_word))),
            #'+' + str(next_index) + (':word.is_to={}'.format(is_to(target_word))),
            
            #'+' + str(next_index) + (':word.is_NN={}'.format(is_NN(target_pos))),
           # '+' + str(next_index) + (':word.is_DT={}'.format(is_DT(target_pos))),
            
        ])
        #if is_input_keyword(target_word):
        #    features.append('+' + str(next_index) + 'word.input_keyword_is=' + target_word)
    #Backward gram
    for k in range(N_gram):
        previous_index = k + 1
        if i - previous_index < 0:
            break
        target_word = sent[i-previous_index][0]
        target_pos = sent[i-previous_index][1]
        features.extend([
            #'-' + str(previous_index) + ':word=' + target_word,
            #'-' + str(previous_index) + (':word.islower={}'.format(target_word.islower())),
            #'-' + str(previous_index) + (':word.isupper={}'.format(target_word.isupper())),
            '-' + str(previous_index) + ':pos=' + target_pos,
            #'-' + str(previous_index) + (':word.is_input_keyword={}'.format(is_input_keyword(target_word))),
            #'-' + str(previous_index) + (':word.is_prep={}'.format(is_prep(target_word))),
            
            #'-' + str(previous_index) + (':word.is_given={}'.format(is_given(target_word))),
            #'-' + str(previous_index) + (':word.is_input={}'.format(is_input(target_word))),
            #'-' + str(previous_index) + (':word.is_give={}'.format(is_give(target_word))),
            #'-' + str(previous_index) + (':word.is_provide={}'.format(is_provide(target_word))),
            #'-' + str(previous_index) + (':word.is_receive={}'.format(is_receive(target_word))),
            #'-' + str(previous_index) + (':word.is_take={}'.format(is_take(target_word))),
            #'-' + str(previous_index) + (':word.is_of={}'.format(is_of(target_word))),
            #'-' + str(previous_index) + (':word.is_for={}'.format(is_for(target_word))),
            #'-' + str(previous_index) + (':word.is_from={}'.format(is_from(target_word))),
            #'-' + str(previous_index) + (':word.is_to={}'.format(is_to(target_word))),
            
            #'-' + str(previous_index) + (':word.is_NN={}'.format(is_NN(target_pos))),
           # '-' + str(previous_index) + (':word.is_DT={}'.format(is_DT(target_pos))),
        
        ])
        #if is_input_keyword(target_word):
        #    features.append('-' + str(previous_index) + 'word.input_keyword_is=' + target_word)
    
    return features

def sent2features(sent):
    return [word2features(sent, i) for i in range(len(sent))]

def convert_label_to_CRF_label(X, Y):
    label_sequence = np.zeros(len(X))
    i = 0
    j = 0
    flag = 0
    start = -1
    s_label = -1
    e_label = -1
    if len(Y) == 0:
        return label_sequence
    while i < len(X):
        if lemmatizer.lemmatize(X[i], pos="v").lower() in input_relateword_sets:
            flag = 1
        if X[i] == Y[j]:
            start = i
            s_label = j
            k = j
            while k < len(Y):
                if Y[k] == ";" or Y[k] == ".":
                    break
                k += 1
            e_label = k
            if X[start:start + (e_label - s_label)] == Y[s_label:e_label]:
                if flag == 1:
                    label_sequence[start:start + (e_label - s_label)] = 1
                    #print(start)
                    #print(start + (e_label - s_label))
                    flag = 0
                    i = start + (e_label - s_label)
                    j = e_label
                else:
                    while X[i] != ".":
                        if lemmatizer.lemmatize(X[i], pos="v").lower() in input_relateword_sets:
                            label_sequence[start:start + (e_label - s_label)] = 1
                            i = start + (e_label - s_label)
                            j = e_label
                            break
                        i += 1
            else:
                i += 1
        else:
            i += 1
        if j >= len(Y):
            break
        if Y[j] == ";" or Y[j] == ".":
            if j == len(Y) - 1:
                break
            j += 1
    return label_sequence

def convert_int_to_str(array):
    str_seq = [str(x) for x in array]
    return str_seq

def convert_CRF2Sen(sentence, y):
    sen = ""
    for i in range(len(y)):
        if y[i] == str(1.0):
            sen += sentence[i] + " "
    return sen



raw_sentences = Read_Sentences(TrainDataPath, "sentences_test3")#augmentation_data#sentences_test3
label_sentences = Read_Sentences(TrainDataPath, "NN_I_test3")#augmentation_data_label#NN_I_test3
train_x = [tokenize(sent) for sent in raw_sentences]
train_y = [tokenize(sent) for sent in label_sentences]


test_sentences = Read_Sentences(TestDataPath, "sentences")
test_label = Read_Sentences(TestDataPath, "NN_I")
test_x = [tokenize(sent) for sent in test_sentences]
test_y = [tokenize(sent) for sent in test_label]
testing_data = [POS(sent) for sent in test_sentences]
X_test = [sent2features(sent) for sent in testing_data]
Y_test_integer = []
for i in range(len(test_x)):
    Y_test_integer.append(convert_label_to_CRF_label(test_x[i], test_y[i]))
Y_test = [convert_int_to_str(seq) for seq in Y_test_integer]
#label_sequence = convert_label_to_CRF_label(train_x[5], train_y[5])
'''
Y_train = []
for i in range(len(train_x)):
    print(i)
    Y_train.append(convert_label_to_CRF_label(train_x[i], train_y[i]))
'''

training_data = [POS(sent) for sent in raw_sentences]
training_data_dep = [DEP(sent) for sent in raw_sentences]
training_label = [tokenize(sent) for sent in label_sentences]
#print(training_data[0][0][0].isupper())
X_train = [sent2features(sent) for sent in training_data]
#Y_train = training_label
Y_train_integer = []
for i in range(len(train_x)):
    Y_train_integer.append(convert_label_to_CRF_label(train_x[i], train_y[i]))
Y_train = [convert_int_to_str(seq) for seq in Y_train_integer]

#Training
'''
model = pycrfsuite.Trainer(verbose=True)
for xseq, yseq in zip(X_train, Y_train):
    model.append(xseq, yseq)
'''
crf = sklearn_crfsuite.CRF(
        c1 = 0,
        c2 = 5e-2,
        max_iterations = 200,
        all_possible_transitions = True
    )

crf.fit(X_train, Y_train)

train_prediction = crf.predict(X_train)
Train_Correct = 0
train_fault = []
train_answer = []
for i in range(len(X_train)):
    if train_prediction[i] == Y_train[i]:
        Train_Correct += 1
    else:
        train_fault.append(i)
        
for i in range(len(X_train)):
    train_sen = convert_CRF2Sen(train_x[i], train_prediction[i])
    train_answer.append(train_sen)

print("Training Accuracy: " + str(Train_Correct/len(X_train)))



prediction = crf.predict(X_test)
Correct = 0
fault = []
answer = []
for i in range(108):
    if prediction[i] == Y_test[i]:
        Correct += 1
    else:
        fault.append(i)
        
for i in range(108):
    sen = convert_CRF2Sen(test_x[i], prediction[i])
    answer.append(sen)


amod_target_index = [10,18,20,30,41,47,55] # [10,18,30,47,55]
Amod_error = [crf.predict_marginals_single(X_test[index]) for index in amod_target_index]
amod_panswer = [answer[index] for index in amod_target_index]
amod_answer = [test_y[index] for index in amod_target_index]

NN_target_index = [1,40,63,70] # [1,40,63,70]
NN_error = [crf.predict_marginals_single(X_test[index]) for index in NN_target_index]
NN_panswer = [answer[index] for index in NN_target_index]
NN_answer = [test_y[index] for index in NN_target_index]

AUX_target_index = [69,76,89,91,98] # [69,76,89,91,98]
AUX_error = [crf.predict_marginals_single(X_test[index]) for index in AUX_target_index]
AUX_panswer = [answer[index] for index in AUX_target_index]
AUX_answer = [test_y[index] for index in AUX_target_index]

case_target_index = [67,105] # [67,105]
case_error = [crf.predict_marginals_single(X_test[index]) for index in case_target_index]
case_panswer = [answer[index] for index in case_target_index]
case_answer = [test_y[index] for index in case_target_index]

print("Testing Accuracy: " + str(Correct/108))
'''
model.set_params({
        'c1': 1.0,
        'c2': 1e-3,
        'max_iterations': 200,
        'feature.possible_transitions': True,
        'feature.minfreq': 3
    })
model.train(model)
#Testing
tagger = pycrfsuite.Tagger()
tagger.open(model)
'''
#y_pred = crf.predict(X_train)
#print(y_pred[0])


nlp.close()
