#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep  7 17:48:58 2020

@author: ken
"""

from stanfordcorenlp import StanfordCoreNLP
import nltk
from nltk.corpus import wordnet as wn
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from operator import is_not
from functools import partial 
import os

#Tool Announcement
nlp = StanfordCoreNLP(r'/home/ken/stanford-corenlp-4.2.0')
#pst = PorterStemmer()
lemmatizer = WordNetLemmatizer()
#Example sentences
'''
sentence_group = ["Request1 specifies a request for compositions that receives the destCity (the name of a city destination for a trip) and DateTime entities as input, and return the Name of a theatre for that city and the receipt for the booking performance (TheatreName, TicketConfirmaton, PlaceNum entities)",
       "The user wishes to provide as inputs a book title and author, credit card information and the address that the book will be shipped to. The outputs of the desired composite service are a payment from the credit card for the purchase, as well as shipping dates and customs costs for the specific item.",
       "Request2 specifies compositions that return HotelConfirmation and RentalConfirmation (the receipt of the booking of a Hotel and a car in city that is the destination for a trip). receiving destCity (the name of the city destination of the trip), ArrivalDateTime (the arrival date), and departDateTime (the departure date) as inputs.",
       "In the scenario the user takes a picture of a barcode of a product and provides his current local position. He wants to obtain the cheapest price offered by a shop close to his position, the GPS coordinate of this shop and a review of the product.",
       "Sa returns the blood pressure (BP) of a patient given his PatientID (PID) and DeviceAddress (Add); Sb and Sb2 will return respectively the supervisor (Person) and a physician of an organisation (Org); Sc returns a Warning level (WL) given a blood pressure; Sd returns the Emergency department given a level of Warning; Se returns the Organization given a Warning level.",
       "PatientByIDService takes a patient ID as input to return the patient’s description. CurrentTreatmentByPatientIDService takes a patient ID to return the patient’s current treatment. MedicalHistoryByPatientIDService takes a patient ID to return the patient’s previous treatments. MedicationByTreatmentIDService takes a treatment ID to return the medication involved in this treatment. DrugclassByMedicaionService takes a medication ID to return the drug class of this medication (indicates the different risks to be associated to the medication).",
       "Given the hotel name, city, and state information, findHotel returns the address and zip code of the hotel.",
       "Given the zip code and food preference, findRestaurant returns the name, phone number, and address of the restaurant with matching food preference and closest to the zip code.",
       "Given the current location and food preference, guideRestaurant returns the address of the closest restaurant and its rating.",
       "Given the start and destination addresses, findDirection returns a detailed step-by-step driving direction and a map image of the destination address.",
       "Book a Hotel and a Theatre in a city where he will arrive in a given date and from which will depart from in another given date.",
       "Overall response time should be less than 200 ms and overall reliability should be more than 90%.",
       "Restricting the overall response time to be less than 50s.",
       "The total price of the composite service execution should be at most $500."
       ]
'''
dir_path = "/home/ken/git_nlp/Service-Requirement-Extraction-via-NLP/src/Leetcode_data/"
fo = open(dir_path + "sentences", "r+")
sentence_group = []
x = fo.readline()
while(x != ""):
    sentence_group.append(x)
    x = fo.readline()


#input and output keywords sets
input_relateword_sets = [
    "input",
    "given",
    "give",
    "provide",
    "receive",
    "take"
    ]

output_relateword_sets = [
    "output",
    "return",
    "obtain",
    "find"
    ]

#Announce Service name
Service_names = []

#Announcement of tuple-based groups
I = []
O = []
P = []
E = []

I_group = []
O_group = []
               
#Get pos_tag, dependency and token
def get_pos_dep_token(sen):
    pos_tag = nlp.pos_tag(sen)
    dep_parse = nlp.dependency_parse(sen)
    tokens = nlp.word_tokenize(sen)
    tokens.insert(0,'ROOT')
    pos_tag.insert(0,('ROOT', 'ROOT'))
    return pos_tag, dep_parse, tokens

#Recover Sentence
def recover_sentence(tokens):
    sen = ''
    #if flag = 1, need to reduce one space
    flag = 0
    for token in tokens:
        if token == 'ROOT':
            continue
        elif token == '_':
            sen = sen + token
            flag = 1
        elif "'" in token:
            sen = sen + token
        else:
            if flag == 1:
                sen = sen + token
                flag = 0
                continue
            sen = sen + ' ' + token
    return sen

def de_coreference(sen, tokens):
    #print(sen)
    coref_information = nlp.coref(sen)
    #return recover_sentence(tokens)
    print(coref_information)
    target_tokens = tokens.copy()
    #print(target_tokens)
    num_of_coref = len(coref_information)
    for i in range(num_of_coref):
        #represent_words = coref_information[i][0][3]
        for k in range(len(coref_information[i])):
            #period_index = 0
            if k == 0:
                if coref_information[i][k][3].lower() == 'your' or coref_information[i][k][3].lower() == 'his' or coref_information[i][k][3].lower() == 'her' or coref_information[i][k][3].lower() == 'you':
                    break
                def_target_index = 0
                if coref_information[i][k][0] > 1:
                    indicator = coref_information[i][k][0] - 1
                #period_index = 0
                    for t_index in range(len(tokens)):
                        if tokens[t_index] == '.':
                            indicator = indicator - 1
                        if indicator == 0:
                            def_target_index = t_index
                            #print(def_target_index)
                            break
                continue
            
            '''
            indicator = coref_information[i][k][0] - 1
            #period_index = 0
            for token in target_tokens:
                if token == '.':
                    indicator = indicator - 1
                if indicator == 0:
                    period_index = target_tokens.index(token)
                    break
            '''
            if "the " + coref_information[i][0][3].lower() + " 's" == coref_information[i][k][3].lower() or coref_information[i][0][3].lower() + " 's" == coref_information[i][k][3].lower():
                continue
            #print("----k = %d\n"%k)
            indicator2 = coref_information[i][k][0] - 1
            #period_index = 0
            for t_index in range(len(tokens)):
                if tokens[t_index] == '.':
                    indicator2 = indicator2 - 1
                if indicator2 == 0:
                    period_index_2 = t_index
                    #print(period_index_2)
                    break
                
            #print(period_index)
            #find target_token index
            nums_words = coref_information[i][k][2] - coref_information[i][k][1]
            #print("----period_index_2 = %d\n"%period_index_2)
            #print("----nums_words = %d\n"%nums_words)
            #print(nums_words)
            flag = 1
            #print(len(target_tokens))
            for y in range(len(target_tokens)):
                #print(y)
                #print(target_tokens[y])
                if target_tokens[y] == tokens[coref_information[i][k][1] + period_index_2]:
                    for x in range(nums_words):
                        flag = 0
                        if target_tokens[y + x] != tokens[coref_information[i][k][1] + period_index_2 + x]:
                            flag = 1
                            break
                if flag == 1:
                    continue
                else:
                    target_index = y
                    #print(y)
                    break
            #print(flag)
            if flag == 1:
                continue
            del target_tokens[target_index:target_index + nums_words]
            #target_index = coref_information[i][k][2] + period_index -  1 + add_index
            tokens_index = coref_information[i][0][2] + def_target_index -  1
            while tokens_index >= coref_information[i][0][1] + def_target_index:
                target_tokens.insert(target_index ,tokens[tokens_index])
                #target_index = target_index - 1
                tokens_index = tokens_index - 1
            #add_index = add_index + (coref_information[i][0][2] - coref_information[i][0][1]) - (coref_information[i][k][2] - coref_information[i][k][1])
        #tokens[coref_information[i][1][1]] = coref_information[i][0][3]
    return recover_sentence(target_tokens)

#Replace unique service name with str - Service
def modify_sentence(tokens, pos_tag):
    flag = 0
    for tag in pos_tag:
        if tag[1] == 'JJ':
            if tag[0][0].islower():
                flag = 1
            for ch in tag[0]:
                if ch.isupper() and flag == 1:
                    flag = 0
                    target_index = tokens.index(tag[0])
                    if tag[0] in Service_names:
                        tokens[target_index] = 'Service' + str(Service_names.index(tag[0]))
                        continue
                    number_of_service = len(Service_names)
                    Service_names.append(tokens[target_index])
                    tokens[target_index] = 'Service' + str(number_of_service)
    #replace - to _
    tokens = ['_' if x == '-' in tokens else x for x in tokens]
    #replace ; to .
    tokens = ['.' if x == ';' in tokens else x for x in tokens]    
    sen = recover_sentence(tokens)
    mod_token = nlp.word_tokenize(sen)
    mod_token.insert(0,'ROOT')
    #print(sen)
    for tag in pos_tag:
        if tag[1] == 'PRP' or tag[1] == 'PRP$':
            sen = de_coreference(sen, mod_token)
            break
    
    return sen

#Find the dependency of a certain word
def find_keyword_relate_dep(word, dep_parse, tokens):
    relate_dep = []
    for dep in dep_parse:
        if dep[0] == 'ROOT':
            continue
        if lemmatizer.lemmatize(tokens[(dep[1])]) == lemmatizer.lemmatize(word) or lemmatizer.lemmatize(tokens[(dep[2])]) == lemmatizer.lemmatize(word):
            relate_dep.append(dep)
    return relate_dep

#Recover the compound
def recover_compound(token_index, dep_parse, tokens, pos_tag):
    comb_sets = []
    for dep in dep_parse:
        ext_word = tokens[token_index]
        if pos_tag[token_index][1] == 'CD' and tokens[token_index+1] == "'s":
            if tokens[token_index-1] == "'":
                ext_word = "'" + ext_word + "'s"
            else:
                ext_word = ext_word + "'s"
        
        if dep[0] == 'compound' and dep[1] == token_index:
            comb_sets.append(dep[2])
            #ext_word = tokens[dep[2]] + '_' + ext_word
            i = 0
            target_index = dep[2]
            while i < len(dep_parse):
                #print(dep_parse[i][1])
                if (dep_parse[i][0] == 'compound' or dep_parse[i][0] == 'amod') and dep_parse[i][1] == target_index:
                    #print(target_index)
                    #ext_word = tokens[dep_parse[i][2]] + '_' + ext_word
                    comb_sets.append(dep_parse[i][2])
                    i = 0
                    target_index = dep_parse[i][2]
                i = i + 1
            #return ext_word
        elif (dep[0] == 'amod' or dep[0] == 'nummod' or dep[0] == 'advmod') and dep[1] == token_index:
            comb_sets.append(dep[2])
        elif dep[0] == 'appos'  and dep[1] == token_index:
            ext_word = ext_word + "_" + tokens[dep[2]]
            
        elif dep[0] == 'det' and dep[1] == token_index and (tokens[dep[2]] != 'a' and tokens[dep[2]] != 'an' and tokens[dep[2]] != 'the'):
            comb_sets.append(dep[2])
    comb_sets = sorted(comb_sets, reverse = True)
    #print("123")
    #print(comb_sets)
    if (len(comb_sets) != 0) and ((comb_sets[0] == token_index - 1) or (comb_sets[0] == token_index + 1)):
        for k in range(len(comb_sets)):
            ext_word = tokens[comb_sets[k]] + '_' + ext_word
        return ext_word
    if pos_tag[token_index-1][1] == 'NN' and pos_tag[token_index][1] == 'NN':
        #return tokens[token_index-1] + '_' + tokens[token_index]
        comb_sets.append(token_index-1)
    elif pos_tag[token_index+1][1] == 'NN' and pos_tag[token_index][1] == 'NN':
        #return tokens[token_index] + '_' + tokens[token_index+1]
        #comb_sets.append(token_index)
        comb_sets.append(token_index+1)
    for k in range(len(comb_sets)):
        if comb_sets[k] < token_index:
            ext_word = tokens[comb_sets[k]] + '_' + ext_word
        else:
            ext_word = ext_word + '_' + tokens[comb_sets[k]]
    return ext_word
    #return tokens[token_index]

#Find conjunction words
def find_conj_words(token_index, dep_parse, tokens, pos_tag, keywords):
    words_index = []
    conj_index = []
    if pos_tag[token_index][1] != 'VBZ':
        words_index.append(token_index)
    
    #print(tokens[token_index])
    list_len = 1
    index = 0
    for dep in dep_parse:
        if dep[0] == 'conj':
            conj_index.append(dep)
        elif dep[0] == 'xcomp' and token_index == dep[1]: #and pos_tag[dep[2]][1] == 'NN':
            if pos_tag[dep[2]][1] == 'JJ':
                continue
            words_index[0] = dep[2]
            #list_len = len(words_index)
    while list_len != index and words_index != []:
        target_index = words_index[index]
        for dep in conj_index:
            if dep[1] == target_index and (pos_tag[dep[1]][1] == pos_tag[dep[2]][1] or pos_tag[dep[2]][1] == 'NNS' or pos_tag[dep[2]][1] == 'NNP' or pos_tag[dep[1]][1] == 'NNS' or pos_tag[dep[2]][1] == 'NN'):
                words_index.append(dep[2])
        index = index + 1
        list_len = len(words_index)
    #Recover the compound word
    #print(words_index)
    for index in words_index:
        #keywords.append(recover_compound(index, dep_parse, tokens, pos_tag))
        find_of_information(index, dep_parse, tokens, pos_tag, keywords)
    
def Check_Case_in(token_index, dep_parse, tokens):
    for dep in dep_parse:
        if dep[0] == 'case' and dep[1] == token_index:
            if tokens[dep[2]] == 'in':
                return 1
            else:
                return 0
    return 0

def Check_Case_of(token_index, dep_parse, tokens):
    for dep in dep_parse:
        if dep[0] == 'case' and dep[1] == token_index:
            if tokens[dep[2]] == 'of':
                return 1
            else:
                return 0
    return 0

#Find comprehensive information of target noun
def find_of_information(token_index, dep_parse, tokens, pos_tag, keywords):
    target_index = -1
    k = 1
    flag = 0
    #print(tokens[token_index])
    extend_word = recover_compound(token_index, dep_parse, tokens, pos_tag)
    #print(extend_word)
    #print(extend_word)
    for dep in dep_parse:
        if dep[0] == 'nmod' and dep[1] == token_index:
            if target_index == -1:
                #print(dep[2])
                target_index = dep[2]
        if dep[0] == 'case' and dep[1] == token_index and pos_tag[dep[2]][1] == 'POS':
            extend_word = extend_word + '_' + tokens[dep[2]]
        
        elif dep[0] == 'nmod:poss' and dep[1] == token_index:
            if tokens[dep[2]+1] == "'s":
                extend_word = tokens[dep[2]] + tokens[dep[2]+1] + '_' + extend_word
            else:
                extend_word = tokens[dep[2]] + '_' + extend_word
        elif dep[0] == 'case' and dep[1] == token_index and (tokens[dep[2]] == 'of' or tokens[dep[2]] == 'for' or tokens[dep[2]] == 'from'):
            k = 0
        elif (dep[0] == 'acl' or dep[0] == 'ccomp') and dep[1] == token_index:
            #print("123")
            details = ""
            details_index = dep[2]
            for dep2 in dep_parse:
                if dep2[0] == 'obj' and dep2[1] == details_index:
                    details = tokens[details_index] + '_' + recover_compound(dep2[2], dep_parse, tokens, pos_tag)
                    break
                elif dep2[0] == 'nsubj:pass' and dep2[1] == details_index:
                    details = tokens[details_index] + '_' + recover_compound(dep2[2], dep_parse, tokens, pos_tag)
                    break
                elif dep2[0] == 'obl' and dep2[1] == details_index:
                    if Check_Case_in(dep2[2], dep_parse, tokens):
                        details = recover_compound(dep2[2], dep_parse, tokens, pos_tag) + '_'
                        flag = 1
                        break
                    elif Check_Case_of(dep2[2], dep_parse, tokens):
                        #print("123")
                        details = tokens[details_index] + '_' +  recover_compound(dep2[2], dep_parse, tokens, pos_tag) + '_'
                        flag = 1
                    else:
                        break
            if not flag:
                extend_word = details + '_' + extend_word
        '''
        elif dep[0] == 'ccomp' and dep[1] == token_index:
            details_index = dep[2]
            details = ""
            for dep2 in dep_parse:
                if dep2[0] == 'nsubj:pass' and dep2[1] == details_index:
                    details = tokens[details_index] + '_' + recover_compound(dep2[2], dep_parse, tokens, pos_tag)
                    break
            extend_word = details + '_' + extend_word
        '''
        '''
        elif dep[0] == 'amod' and dep[1] == token_index:
            extend_word = tokens[dep[2]] + '_' + extend_word
        '''
    #print(target_index)
    if k:#target_index == -1 and k:
        if not (lemmatizer.lemmatize(extend_word, pos="v").lower() in input_relateword_sets or lemmatizer.lemmatize(extend_word, pos="v").lower() in output_relateword_sets):
            #keywords.append(extend_word)
            
            if len(keywords) != 0:
                if not (extend_word in keywords[-1]):
                    keywords.append(extend_word)
            else:        
                keywords.append(extend_word)
            
            #print(extend_word)
            #print("I'm right now here")
        #keywords.append(extend_word)
    #Check nmod is related to of condition
    for dep in dep_parse:
        if dep[0] == 'case' and dep[1] == target_index and (tokens[dep[2]] == 'of' or tokens[dep[2]] == 'for' or tokens[dep[2]] == 'from' or tokens[dep[2]] == 'towards' or tokens[dep[2]] == 'per' or tokens[dep[2]] == 'at'  or tokens[dep[2]] == 'with'):
            #print(extend_word)
            #keywords.remove(extend_word)
            previous_words = keywords.pop()
            if tokens[dep[2]] == 'for':
                extend_word = previous_words + '_for_' + recover_compound(target_index, dep_parse, tokens, pos_tag)
            elif tokens[dep[2]] == 'at':
                extend_word = previous_words + '_at_' + recover_compound(target_index, dep_parse, tokens, pos_tag)
                temp = target_index
                target_index = -1
                for dep2 in dep_parse:
                    if dep2[0] == 'conj' and dep2[1] == temp:
                        target_index = dep2[2]
                        break
                del temp
            elif tokens[dep[2]] == 'towards':
                extend_word = previous_words + '_towards_' + recover_compound(target_index, dep_parse, tokens, pos_tag)
                temp = target_index
                target_index = -1
                for dep2 in dep_parse:
                    if dep2[0] == 'conj' and dep2[1] == temp:
                        target_index = dep2[2]
                        break
                del temp
            elif tokens[dep[2]] == 'per':
                extend_word = previous_words + '_per_' + recover_compound(target_index, dep_parse, tokens, pos_tag)
                temp = target_index
                target_index = -1
                for dep2 in dep_parse:
                    if dep2[0] == 'conj' and dep2[1] == temp:
                        target_index = dep2[2]
                        break
                del temp
            elif flag:
                extend_word = details + recover_compound(target_index, dep_parse, tokens, pos_tag) + "_" + previous_words
                flag = 0
            else:
                extend_word = recover_compound(target_index, dep_parse, tokens, pos_tag) + "_" + previous_words
            if not (lemmatizer.lemmatize(extend_word, pos="v").lower() in input_relateword_sets or lemmatizer.lemmatize(extend_word, pos="v").lower() in output_relateword_sets):
                keywords.append(extend_word)
                
            #keywords.append(extend_word)
            if target_index != -1:
                find_conj_words(target_index, dep_parse, tokens, pos_tag, keywords)
                #target_index = -1
        elif dep[0] == 'case' and target_index == -1 and tokens[dep[2]] == 'for':
            if not (lemmatizer.lemmatize(extend_word, pos="v").lower() in input_relateword_sets or lemmatizer.lemmatize(extend_word, pos="v").lower() in output_relateword_sets):
                keywords.append(extend_word)
        elif flag:
            previous_words = keywords.pop()
            keywords.append(details + previous_words)
            flag = 0
            
            

#Find the target noun
def find_noun_components(relate_dep, dep_parse, tokens, pos_tag):
    keywords  = []
    for target in relate_dep:
        #find the noun that has dependency with key verb
        if target[0] == 'case':
            target_noun_index = target[1]
            #keywords.append(find_of_information(target_noun_index, dep_parse, tokens, pos_tag))
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
        elif target[0] == 'obj' or target[0] == 'iobj':
            target_noun_index = target[2]
            #keywords.append(find_of_information(target_noun_index, dep_parse, tokens, pos_tag))
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
        elif target[0] == 'conj':
            if pos_tag[target[1]][1] == 'VBZ' and pos_tag[target[2]][1] == 'VBZ':
                continue
            if (((lemmatizer.lemmatize(tokens[target[1]], pos="v")) in input_relateword_sets) and ((lemmatizer.lemmatize(tokens[target[2]], pos="v")) in output_relateword_sets)) or (((lemmatizer.lemmatize(tokens[target[2]], pos="v")) in input_relateword_sets) and ((lemmatizer.lemmatize(tokens[target[1]], pos="v")) in output_relateword_sets)):
                continue
            target_noun_index = target[2]
            #keywords.append(find_of_information(target_noun_index, dep_parse, tokens, pos_tag))
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
        elif target[0] == 'nsubj':
            target_noun_index = target[1]
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
        elif target[0] == 'acl':
            target_noun_index = target[1]
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
        elif target[0] == 'nsubj:pass':
            target_noun_index = target[2]
            if pos_tag[target_noun_index][1] == 'PRP':
                continue
            flag = 0
            for dep2 in dep_parse:
                if dep2[0] == 'obj' and dep2[1] == target[1]:
                    flag = 1
                    break
            if flag == 1:
                del flag
                continue
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
            del flag
        elif target[0] == 'amod':
            target_noun_index = target[1]
            if I == [] and lemmatizer.lemmatize(tokens[target[2]], pos="v").lower() in input_relateword_sets:
                keywords.append(recover_compound(target[1], dep_parse, tokens, pos_tag))
            elif O == [] and lemmatizer.lemmatize(tokens[target[1]], pos="v").lower() in output_relateword_sets:
                keywords.append(recover_compound(target[2], dep_parse, tokens, pos_tag))
        elif target[0] == 'parataxis':
            target_noun_index = target[1]
            for dep in dep_parse:
                if dep[0] == 'obl' and dep[1] == target_noun_index:
                    target_noun_index = dep[2]
            if I == [] and lemmatizer.lemmatize(tokens[target[2]], pos="v").lower() in input_relateword_sets:
                find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
            elif O == [] and lemmatizer.lemmatize(tokens[target[1]], pos="v").lower() in output_relateword_sets:
                find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
        elif target[0] == 'mark':
            target_noun_index = target[1]
            if lemmatizer.lemmatize(tokens[target_noun_index], pos="v").lower() in input_relateword_sets or lemmatizer.lemmatize(tokens[target_noun_index], pos="v").lower() in output_relateword_sets:
                continue
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
        elif target[0] == 'obl':
            for dep in dep_parse:
                if (dep[0] == 'obj' or dep[0] == 'obl:tmod') and dep[1] == target[1]:
                    if target[1] > target[2]:
                        break
                    target_noun_index = dep[2]
                    find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
                #elif dep[0] == 'obl:tmod' and dep[1] == target[1]
        elif target[0] == 'nmod':
            target_noun_index = target[1]
            find_conj_words(target_noun_index, dep_parse, tokens, pos_tag, keywords)
            
    return keywords

#Remove nsubj exceptions
def remove_nsubj(relate_dep):
    for dep in relate_dep:
        if dep[0] == 'nsubj':
            relate_dep.remove(dep)
        '''
        elif dep[0] == 'case' and ((lemmatizer.lemmatize(tokens[dep[1]], pos="v") in input_relateword_sets) or (lemmatizer.lemmatize(tokens[dep[1]], pos="v") in output_relateword_sets)):
            relate_dep.remove(dep)
        elif dep[0] == 'xcomp' and ((lemmatizer.lemmatize(tokens[dep[1]], pos="v") in input_relateword_sets) or (lemmatizer.lemmatize(tokens[dep[1]], pos="v") in output_relateword_sets)):
            relate_dep.remove(dep)
        '''
    #print(relate_dep)
    return relate_dep

#Remove case exceptions
def remove_case(relate_dep, tokens):
    for dep in relate_dep:
        if dep[0] == 'case' and ((lemmatizer.lemmatize(tokens[dep[1]], pos="v") in input_relateword_sets) or (lemmatizer.lemmatize(tokens[dep[1]], pos="v") in output_relateword_sets)):
            relate_dep.remove(dep)
    #print(relate_dep)
    return relate_dep

def find_verb_conj(token_index, dep_parse, tokens, pos_tag):
    words_index = []
    conj_index = []
    words_index.append(token_index)
    list_len = 1
    index = 0
    for dep in dep_parse:
        if dep[0] == 'conj':
            conj_index.append(dep)
        elif dep[0] == 'xcomp':
            if ((lemmatizer.lemmatize(tokens[dep[1]], pos="v")).lower() in input_relateword_sets) and ((lemmatizer.lemmatize(tokens[dep[2]], pos="v")).lower() in output_relateword_sets):
                continue
            elif ((lemmatizer.lemmatize(tokens[dep[2]], pos="v")).lower() in input_relateword_sets) and ((lemmatizer.lemmatize(tokens[dep[1]], pos="v")).lower() in output_relateword_sets):
                continue
            conj_index.append(dep)
    while list_len != index:
        target_index = words_index[index]
        for dep in conj_index:
            if dep[1] == target_index and pos_tag[dep[2]][1] == 'VBZ':
                words_index.append(dep[2])
            elif dep[2] == target_index and pos_tag[dep[1]][1] == 'VBZ':
                words_index.append(dep[1])
            conj_index.remove(dep)
        index = index + 1
        list_len = len(words_index)
    return words_index

def check_case_obj(relate_dep):
    for dep in relate_dep:
        if dep[0] == 'case' or dep[0] == 'obj' or dep[0] == 'conj' or dep[0] == 'nsubj' or dep[0] == 'iobj' or dep[0] == 'amod' or dep[0] == 'nsubj:pass' or dep[0] == 'parataxis' or dep[0] == 'acl':
            return 1
    return 0

def check_used_conjwords(token_index, tokens, relate_dep):
    for dep in relate_dep:
        if (dep[0] == 'conj' and dep[2] == token_index) or (dep[0] == 'case' and dep[1] == token_index):
            if ((lemmatizer.lemmatize(tokens[dep[1]], pos="v")).lower() in output_relateword_sets) and ((lemmatizer.lemmatize(tokens[dep[2]], pos="v")).lower() in output_relateword_sets):
                return 1
            elif ((lemmatizer.lemmatize(tokens[dep[1]], pos="v")).lower() in input_relateword_sets) and ((lemmatizer.lemmatize(tokens[dep[2]], pos="v")).lower() in input_relateword_sets):
                return 1
    return 0



#output_modisen = open("./mod_sentence", "w")

for sen_index in range(len(sentence_group)):
    sen = sentence_group[60]
    print("-----------Original sentence-----------\n" + sen + "\n ------------------------------------ \n")
    pos_tag, dep_parse, tokens = get_pos_dep_token(sen)
    origin_dep_parse = dep_parse.copy()
    origin_pos_tag =pos_tag.copy()
    origin_tokens = tokens.copy()
    #print(sen_index)
    sen = modify_sentence(tokens, pos_tag)
    print(str(sen_index + 1) + "\n ------------------------------------ \n")
    print(sen + "\n ------------------------------------ \n")
    #sen = " The user wishes to provide as inputs a book title and author , credit card information and the address that the book will be shipped to ."
    #pos_tag, dep_parse, tokens = get_pos_dep_token(sen)
    #output_modisen.writelines(sen)
    #output_modisen.write("\n")

    #Sentence tokenizer
    sen_token = nltk.sent_tokenize(sen)
    for i in range(len(sen_token)):
        pos_tag, dep_parse, tokens = get_pos_dep_token(sen_token[i])
        bracket_flag = 0
        for token in tokens:
            if token == '(':
                bracket_flag = 1
            elif token == ')':
                bracket_flag = 0
            if bracket_flag:
                continue
            
            if (lemmatizer.lemmatize(token, pos="v")).lower() in input_relateword_sets and pos_tag[tokens.index(token)][1] != 'NNS':
                input_relate_dep = find_keyword_relate_dep(token, dep_parse, tokens)
                if check_used_conjwords(tokens.index(token), tokens, dep_parse):
                    continue
                i_verb_list = find_verb_conj(tokens.index(token),dep_parse, tokens, pos_tag)
                for index in i_verb_list:
                    input_relate_dep = find_keyword_relate_dep(tokens[index], dep_parse, tokens)
                    if pos_tag[index][1] == 'VBZ' or pos_tag[index][1] == 'VBP' or pos_tag[index][1] == 'VB':
                        input_relate_dep = remove_nsubj(input_relate_dep)
                    #if pos_tag[index][1] == 'NN':
                    #    continue
                    if not check_case_obj(input_relate_dep):
                        continue
                    #input_relate_dep = remove_case(input_relate_dep, tokens)
                    #I = find_noun_components(input_relate_dep, dep_parse, tokens, pos_tag)
                    temp = find_noun_components(input_relate_dep, dep_parse, tokens, pos_tag)
                    temp = list(dict.fromkeys(temp))
                    if temp != []:
                        I.append(temp)
                    del temp
            elif (lemmatizer.lemmatize(token, pos="v")).lower() in output_relateword_sets:
                output_relate_dep = find_keyword_relate_dep(token, dep_parse, tokens)
                if check_used_conjwords(tokens.index(token), tokens, dep_parse):
                    continue
                '''
                if pos_tag[tokens.index(token)][1] == 'VBZ' or pos_tag[tokens.index(token)][1] == 'VBP':
                    output_relate_dep = remove_nsubj(output_relate_dep)
                #O = find_noun_components(output_relate_dep, dep_parse, tokens, pos_tag)
                O.append(find_noun_components(output_relate_dep, dep_parse, tokens, pos_tag))
                '''
                o_verb_list = find_verb_conj(tokens.index(token),dep_parse, tokens, pos_tag)
                for index in o_verb_list:
                    output_relate_dep = find_keyword_relate_dep(tokens[index], dep_parse, tokens)
                    if pos_tag[index][1] == 'VBZ' or pos_tag[index][1] == 'VBP' or pos_tag[index][1] == 'VB':
                        output_relate_dep = remove_nsubj(output_relate_dep)
                    #if pos_tag[index][1] == 'NN':
                    #    continue
                    if not check_case_obj(output_relate_dep):
                        continue
                    #output_relate_dep = remove_case(output_relate_dep, tokens)
                    #I = find_noun_components(input_relate_dep, dep_parse, tokens, pos_tag)
                    #O.append(find_noun_components(output_relate_dep, dep_parse, tokens, pos_tag))
                    temp = find_noun_components(output_relate_dep, dep_parse, tokens, pos_tag)
                    temp = list(dict.fromkeys(temp))
                    if temp != []:
                        O.append(temp)
                    del temp
        #print(i)
        #break
    #os.system("pause")
    #input()
    I_group.append(I)
    O_group.append(O)
    break
    #if sen_index == 30:
    #    break
    I = []
    O = []
nlp.close()
#output_modisen.close()